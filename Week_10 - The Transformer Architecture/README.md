# ðŸŽ¯ Goals for week 10

1. Implement an encoder-decoder Transformer.
2. Practice writing high quality code:
   1. Easy to read.
   2. Safe from bugs.
   3. Ready for change.

## Task 01

**Description:**

Define a transformer with `8` attention heads, `6` encoder and decoder layers, and for input sequence embeddings of length `1536`. Output the model object to show the model architecture.

**Acceptance criteria:**

1. A Transformer is created using a built-in class.
2. The test case passes.

**Test case:**

```console
python task01.py
```

```console
Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
        )
        (linear1): Linear(in_features=1536, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=1536, bias=True)
        (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoder(
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
        )
        (multihead_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)
        )
        (linear1): Linear(in_features=1536, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=1536, bias=True)
        (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  )
)
```

## Task 02

**Description:**

Define a class `InputEmbeddings` that can encode `10,000` tokens into `512`-dimensional vectors.

Apply it on the following input and show the output's shape and the embedding of the first token.

```python
token_ids = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])
```

Set the seed of PyTorch to `123`.

**Acceptance criteria:**

1. An input embedding is implemented as shown in the Transformer paper and in `notes.md`.
2. The test case passes.
3. A seed is set for reproducibility.

**Test case:**

```console
python task02.py
```

```console
Shape of output: torch.Size([2, 4, 512])
Embedding of first token: tensor([-29.7585,  -1.5320,  -3.0543, -11.7277,   5.2621,  36.9068, -19.7024,
          1.1091,  -5.7159,   1.8338,  28.7414,  16.7375,  24.6407, -14.9143,
         -4.3227,  11.4514,  40.2486, -16.0469, -34.5345, -33.2455,  72.5264,
         -6.2573,  -1.2220,  15.0982, -11.3665, -24.9873,  19.7784, -33.3697,
         -1.7752, -20.8495, -27.1865,  -3.7119,  25.7179, -16.5864,  30.2809,
          6.1220,  11.4742,   0.3861,  -6.9708, -26.4641,  -9.6485, -20.9665,
          9.2181,  -1.4408, -31.9163, -26.4945, -24.7647,  -8.3727,  -0.0832,
         14.2537,  11.7048,  13.6467, -22.7297,   4.4323,  32.7201, -55.6621,
        -16.1116, -39.4413,   1.1559,  -6.2625, -13.9900,  10.1023,  45.5336,
        -52.6108,  28.7381, -11.9389,  21.9032,  13.7663,   4.9452, -44.5905,
        -15.0232,  -0.5866,  28.7974, -18.0050,   2.6966, -18.4220,   3.0125,
          4.8027, -19.9902,   6.5366,   4.6368,  19.9776, -21.6073, -19.5977,
         36.0587,  38.1529,  -7.7735,  16.5344,  13.2635,  14.4091, -21.1238,
        -12.5740,  -4.3704,  -3.6633, -34.2731, -55.4303, -33.4379,  13.5223,
         -1.1283,  -7.8512,  -8.8048,  42.8758,   4.6932,   9.4659,  14.6143,
        -21.2894,  -5.6484, -17.5059,   1.7041,  11.8412,  15.6611,   4.9840,
        -16.8916,   5.0464,   0.2251, -14.4334, -38.8943,  32.8271,  21.2031,
        -41.6533,  12.5337,  43.3777, -13.7430, -16.3388,  57.3104, -28.6661,
         26.8866,   2.7629, -25.0370,  28.6968,   7.1197, -22.6045, -11.9889,
        -25.4726, -35.8391,   2.9377,  42.1359, -16.8475,   8.1163,  19.5117,
         20.7159,  24.6284,  16.5404, -12.3744, -40.7849, -11.8295,  18.4424,
         -3.1644,  15.7504,   6.9601, -25.8851, -12.1223,  -7.1028,  16.0071,
        -22.2798,   5.2601, -11.5598, -23.8716,   1.7994,  27.7337,  39.5976,
         -4.5922, -15.5098,  32.2888,  -7.3596,  15.8680,  -2.8552, -29.2617,
         10.9933,  26.4579,   4.9495, -21.5847,  27.1617,  61.1840,   0.5942,
         11.8184,   6.8823, -26.1007, -38.4629,  15.9819,  -5.4510,   7.7299,
        -30.9505, -25.3766,  -8.0396,   7.6134,  -2.3658,  -9.3770,  -9.0279,
         10.1050, -20.2966,   2.2822,   5.3035, -45.1953, -29.6070,  -0.6848,
         -4.4562, -35.6545,   0.9710, -29.6781,  12.8074, -26.1750, -21.4090,
        -18.6798,   6.0818,   1.4804,   8.5436,  -7.7523,  12.7727,  23.7027,
         49.5719, -15.3754,  -4.9656,  24.1539,  -2.7429,   6.5723,  24.6985,
         47.4025, -40.6604, -17.4845,  17.9730,   1.0917, -13.5104,  16.7260,
         -2.0898,  -3.2531,  18.7542, -30.6544,  20.8217,   3.9441, -10.8512,
         -1.6648,  -0.9470,  -1.2108,  27.4451,   2.7254,  16.9774,  -2.2942,
         31.5399,  -8.7147,   5.3491,  20.8418,   2.9770,  10.8673, -25.5511,
         24.2558,  25.0116, -46.5798,   6.5116, -25.7700,   3.9117,  21.8263,
         18.4028,  -7.1983, -31.5330,  11.8256,   5.8349,   7.7378, -18.4817,
         37.9504, -18.9006,  17.0415,   1.8574,  26.3620, -15.0141, -17.6688,
         -5.1359,  -9.8607,  18.5753, -14.3763,  -9.9239, -10.1201,  34.1638,
         -1.7723,  17.4385,  11.7211,   5.5610,   8.9081, -17.8356,   7.3037,
        -16.8501,   7.0195, -33.0792,  -3.9492, -12.4048,  -9.2706,  -1.4197,
          0.3953,  31.0336,  -5.0378,  23.9081,   8.3421,  41.5415,  29.3176,
         18.2028, -14.0019, -26.6899,   7.6548,  21.0862,  16.8259,   5.6342,
        -31.2575, -18.0672,  14.4121, -35.1413, -36.8650,  13.8188,  28.7779,
        -21.3188,  -6.0357,  -7.2779,  10.1919,   8.4129,  14.6110,  13.1334,
          8.4895,   9.7129, -16.4627, -12.5066,  14.0045, -32.3201,  12.7087,
         17.4255,   8.0693,  -2.8678,  21.3760,   3.3181,   6.0479,  21.4222,
         -3.1814,   0.7435, -48.7430,  31.5713,  26.8030,  -2.8404,   5.6942,
         29.5979,   3.3838,  25.6034,   4.6258,  28.1270,  -0.9262,  16.9493,
          6.8474, -17.1758, -21.5911, -36.5668,  -2.2993, -26.0442, -41.2149,
         26.3716, -52.7374,  11.4536, -34.7630, -13.8615,  38.7084,   6.9553,
         11.7617,  18.0983,  42.4875,  23.3845,   6.8770,  22.6996,   6.4814,
          2.4246,   7.4932,   9.1544,  29.7064,  -2.5937, -29.8130,  16.0086,
          5.7597,  -6.5396,  -4.8443,  30.8642, -13.7508,  23.4983, -49.4736,
          3.3504, -19.1815,  17.4893, -30.7139, -26.5765, -11.5998,   3.5777,
         20.5107, -41.7281,   4.3692, -48.8849,   5.2804,   0.4861,  42.2847,
          0.6365,   5.0782,  30.5444,   4.6193,  12.7664, -10.1332, -22.5422,
         12.2599, -14.1985,   5.6465,  14.1219,  32.9376,  30.2804,  -2.7037,
         25.4789,   6.2272, -16.4246,  10.0101,   2.5051,  16.5509,  -5.3302,
         61.5765, -23.1225,  11.0392, -41.3581, -14.5705,  18.5307,   0.9182,
         13.0106,  -8.9994, -22.2943, -13.9907,  37.2299,   4.5012, -26.6634,
          0.1506,  13.6049, -19.3432,  11.6595,  12.0133, -12.4869,  19.5970,
          3.8460,  44.7983, -27.3770,   1.6130,  18.6860,  -0.5926,  22.5234,
         53.1122,  -2.5404,  -8.2858, -23.7794, -15.2391, -13.0401, -21.0235,
        -17.5797, -31.1299,  -0.9840,  66.9112, -28.1762,  17.3499, -30.3137,
        -13.1691,  75.0020, -17.8242,   1.4963,  67.7828,   3.8089,  -3.4556,
          0.2967,  -5.2298,  24.7874, -12.7758,  20.2924, -10.4120,  35.6871,
         62.0738,  63.2288, -17.5947,   8.8316,  -7.5371, -21.0168,  -6.7385,
          4.7280,  -7.3658,  13.2677, -13.8512, -61.0264,  -5.6877, -20.1770,
         18.6802,  21.7048, -35.6451, -15.4166,  -9.4611,   5.8211,  15.6959,
          9.5188,  24.0897,  -0.5046,  13.1973, -27.5199, -23.7867,  -9.7312,
         21.1321, -10.6853,  19.2558,   4.5039,  17.6202, -15.8350,   0.8767,
        -41.4365,  -1.1694,  -0.7806, -14.6271,  -8.1019, -10.9509,  -6.1383,
         -1.7513], grad_fn=<SelectBackward0>)
```

## Task 03

**Description:**

Define a class `PositionalEncoding` that can positionally encode sequences of `4` token IDs into `512`-dimensional vectors.

Apply it on the output of `Task 2` and print the shape of the result and the encoding of the first token embedding.

Set the seed of PyTorch to `123`.

In the `__init__` we'll have to create a vector with the values of the sine and cosine functions. We'll need to use this vector in the `forward` call to add it to the received embeddings. If we store the vector with the positional encodings directly in `self`, PyTorch will treat it as a set of learnable parameters. One way to turn this off is the using the function [`register_buffer`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer). It's used to tell PyTorch to not consider a set of numbers a learnable parameter.

The goal is to have something like this:

```python
def __init__(self, d_model, max_seq_length):
    super().__init__()
    # create "pe" here (don't store it in self explicitly)
    self.register_buffer('pe', pe.unsqueeze(0)) # use "register_buffer" to put it in self

def forward(self, x):
    return x + self.pe[:, :x.size(1)] # use as if it was in self
```

**Acceptance criteria:**

1. Positional encoding is implemented as shown in the Transformer paper and in `notes.md`.
2. The test case passes.
3. A seed is set for reproducibility.

**Test case:**

```console
python task03.py
```

```console
Shape of output: torch.Size([2, 4, 512])
Encoding of first token embedding: tensor([-2.9758e+01, -5.3198e-01, -3.0543e+00, -1.0728e+01,  5.2621e+00,
         3.7907e+01, -1.9702e+01,  2.1091e+00, -5.7159e+00,  2.8338e+00,
         2.8741e+01,  1.7738e+01,  2.4641e+01, -1.3914e+01, -4.3227e+00,
         1.2451e+01,  4.0249e+01, -1.5047e+01, -3.4534e+01, -3.2246e+01,
         7.2526e+01, -5.2573e+00, -1.2220e+00,  1.6098e+01, -1.1367e+01,
        -2.3987e+01,  1.9778e+01, -3.2370e+01, -1.7752e+00, -1.9850e+01,
        -2.7187e+01, -2.7119e+00,  2.5718e+01, -1.5586e+01,  3.0281e+01,
         7.1220e+00,  1.1474e+01,  1.3861e+00, -6.9708e+00, -2.5464e+01,
        -9.6485e+00, -1.9967e+01,  9.2181e+00, -4.4082e-01, -3.1916e+01,
        -2.5495e+01, -2.4765e+01, -7.3727e+00, -8.3172e-02,  1.5254e+01,
         1.1705e+01,  1.4647e+01, -2.2730e+01,  5.4323e+00,  3.2720e+01,
        -5.4662e+01, -1.6112e+01, -3.8441e+01,  1.1559e+00, -5.2625e+00,
        -1.3990e+01,  1.1102e+01,  4.5534e+01, -5.1611e+01,  2.8738e+01,
        -1.0939e+01,  2.1903e+01,  1.4766e+01,  4.9452e+00, -4.3591e+01,
        -1.5023e+01,  4.1338e-01,  2.8797e+01, -1.7005e+01,  2.6966e+00,
        -1.7422e+01,  3.0125e+00,  5.8027e+00, -1.9990e+01,  7.5366e+00,
         4.6368e+00,  2.0978e+01, -2.1607e+01, -1.8598e+01,  3.6059e+01,
         3.9153e+01, -7.7735e+00,  1.7534e+01,  1.3264e+01,  1.5409e+01,
        -2.1124e+01, -1.1574e+01, -4.3704e+00, -2.6633e+00, -3.4273e+01,
        -5.4430e+01, -3.3438e+01,  1.4522e+01, -1.1283e+00, -6.8512e+00,
        -8.8048e+00,  4.3876e+01,  4.6932e+00,  1.0466e+01,  1.4614e+01,
        -2.0289e+01, -5.6484e+00, -1.6506e+01,  1.7041e+00,  1.2841e+01,
         1.5661e+01,  5.9840e+00, -1.6892e+01,  6.0464e+00,  2.2508e-01,
        -1.3433e+01, -3.8894e+01,  3.3827e+01,  2.1203e+01, -4.0653e+01,
         1.2534e+01,  4.4378e+01, -1.3743e+01, -1.5339e+01,  5.7310e+01,
        -2.7666e+01,  2.6887e+01,  3.7629e+00, -2.5037e+01,  2.9697e+01,
         7.1197e+00, -2.1605e+01, -1.1989e+01, -2.4473e+01, -3.5839e+01,
         3.9377e+00,  4.2136e+01, -1.5848e+01,  8.1163e+00,  2.0512e+01,
         2.0716e+01,  2.5628e+01,  1.6540e+01, -1.1374e+01, -4.0785e+01,
        -1.0830e+01,  1.8442e+01, -2.1644e+00,  1.5750e+01,  7.9601e+00,
        -2.5885e+01, -1.1122e+01, -7.1028e+00,  1.7007e+01, -2.2280e+01,
         6.2601e+00, -1.1560e+01, -2.2872e+01,  1.7994e+00,  2.8734e+01,
         3.9598e+01, -3.5922e+00, -1.5510e+01,  3.3289e+01, -7.3596e+00,
         1.6868e+01, -2.8552e+00, -2.8262e+01,  1.0993e+01,  2.7458e+01,
         4.9495e+00, -2.0585e+01,  2.7162e+01,  6.2184e+01,  5.9418e-01,
         1.2818e+01,  6.8823e+00, -2.5101e+01, -3.8463e+01,  1.6982e+01,
        -5.4510e+00,  8.7299e+00, -3.0951e+01, -2.4377e+01, -8.0396e+00,
         8.6134e+00, -2.3658e+00, -8.3770e+00, -9.0279e+00,  1.1105e+01,
        -2.0297e+01,  3.2822e+00,  5.3035e+00, -4.4195e+01, -2.9607e+01,
         3.1517e-01, -4.4562e+00, -3.4655e+01,  9.7102e-01, -2.8678e+01,
         1.2807e+01, -2.5175e+01, -2.1409e+01, -1.7680e+01,  6.0818e+00,
         2.4804e+00,  8.5436e+00, -6.7523e+00,  1.2773e+01,  2.4703e+01,
         4.9572e+01, -1.4375e+01, -4.9656e+00,  2.5154e+01, -2.7429e+00,
         7.5723e+00,  2.4699e+01,  4.8402e+01, -4.0660e+01, -1.6484e+01,
         1.7973e+01,  2.0917e+00, -1.3510e+01,  1.7726e+01, -2.0898e+00,
        -2.2531e+00,  1.8754e+01, -2.9654e+01,  2.0822e+01,  4.9441e+00,
        -1.0851e+01, -6.6485e-01, -9.4696e-01, -2.1077e-01,  2.7445e+01,
         3.7254e+00,  1.6977e+01, -1.2942e+00,  3.1540e+01, -7.7147e+00,
         5.3491e+00,  2.1842e+01,  2.9770e+00,  1.1867e+01, -2.5551e+01,
         2.5256e+01,  2.5012e+01, -4.5580e+01,  6.5116e+00, -2.4770e+01,
         3.9117e+00,  2.2826e+01,  1.8403e+01, -6.1983e+00, -3.1533e+01,
         1.2826e+01,  5.8349e+00,  8.7378e+00, -1.8482e+01,  3.8950e+01,
        -1.8901e+01,  1.8041e+01,  1.8574e+00,  2.7362e+01, -1.5014e+01,
        -1.6669e+01, -5.1359e+00, -8.8607e+00,  1.8575e+01, -1.3376e+01,
        -9.9239e+00, -9.1201e+00,  3.4164e+01, -7.7226e-01,  1.7439e+01,
         1.2721e+01,  5.5610e+00,  9.9081e+00, -1.7836e+01,  8.3037e+00,
        -1.6850e+01,  8.0195e+00, -3.3079e+01, -2.9492e+00, -1.2405e+01,
        -8.2706e+00, -1.4197e+00,  1.3953e+00,  3.1034e+01, -4.0378e+00,
         2.3908e+01,  9.3421e+00,  4.1541e+01,  3.0318e+01,  1.8203e+01,
        -1.3002e+01, -2.6690e+01,  8.6548e+00,  2.1086e+01,  1.7826e+01,
         5.6342e+00, -3.0258e+01, -1.8067e+01,  1.5412e+01, -3.5141e+01,
        -3.5865e+01,  1.3819e+01,  2.9778e+01, -2.1319e+01, -5.0357e+00,
        -7.2779e+00,  1.1192e+01,  8.4129e+00,  1.5611e+01,  1.3133e+01,
         9.4895e+00,  9.7129e+00, -1.5463e+01, -1.2507e+01,  1.5004e+01,
        -3.2320e+01,  1.3709e+01,  1.7425e+01,  9.0693e+00, -2.8678e+00,
         2.2376e+01,  3.3181e+00,  7.0479e+00,  2.1422e+01, -2.1814e+00,
         7.4352e-01, -4.7743e+01,  3.1571e+01,  2.7803e+01, -2.8404e+00,
         6.6942e+00,  2.9598e+01,  4.3838e+00,  2.5603e+01,  5.6258e+00,
         2.8127e+01,  7.3844e-02,  1.6949e+01,  7.8474e+00, -1.7176e+01,
        -2.0591e+01, -3.6567e+01, -1.2993e+00, -2.6044e+01, -4.0215e+01,
         2.6372e+01, -5.1737e+01,  1.1454e+01, -3.3763e+01, -1.3862e+01,
         3.9708e+01,  6.9553e+00,  1.2762e+01,  1.8098e+01,  4.3487e+01,
         2.3384e+01,  7.8770e+00,  2.2700e+01,  7.4814e+00,  2.4246e+00,
         8.4932e+00,  9.1544e+00,  3.0706e+01, -2.5937e+00, -2.8813e+01,
         1.6009e+01,  6.7597e+00, -6.5396e+00, -3.8443e+00,  3.0864e+01,
        -1.2751e+01,  2.3498e+01, -4.8474e+01,  3.3504e+00, -1.8181e+01,
         1.7489e+01, -2.9714e+01, -2.6576e+01, -1.0600e+01,  3.5777e+00,
         2.1511e+01, -4.1728e+01,  5.3692e+00, -4.8885e+01,  6.2804e+00,
         4.8614e-01,  4.3285e+01,  6.3652e-01,  6.0782e+00,  3.0544e+01,
         5.6193e+00,  1.2766e+01, -9.1332e+00, -2.2542e+01,  1.3260e+01,
        -1.4198e+01,  6.6465e+00,  1.4122e+01,  3.3938e+01,  3.0280e+01,
        -1.7037e+00,  2.5479e+01,  7.2272e+00, -1.6425e+01,  1.1010e+01,
         2.5051e+00,  1.7551e+01, -5.3302e+00,  6.2576e+01, -2.3123e+01,
         1.2039e+01, -4.1358e+01, -1.3570e+01,  1.8531e+01,  1.9182e+00,
         1.3011e+01, -7.9994e+00, -2.2294e+01, -1.2991e+01,  3.7230e+01,
         5.5012e+00, -2.6663e+01,  1.1506e+00,  1.3605e+01, -1.8343e+01,
         1.1659e+01,  1.3013e+01, -1.2487e+01,  2.0597e+01,  3.8460e+00,
         4.5798e+01, -2.7377e+01,  2.6130e+00,  1.8686e+01,  4.0737e-01,
         2.2523e+01,  5.4112e+01, -2.5404e+00, -7.2858e+00, -2.3779e+01,
        -1.4239e+01, -1.3040e+01, -2.0024e+01, -1.7580e+01, -3.0130e+01,
        -9.8395e-01,  6.7911e+01, -2.8176e+01,  1.8350e+01, -3.0314e+01,
        -1.2169e+01,  7.5002e+01, -1.6824e+01,  1.4963e+00,  6.8783e+01,
         3.8089e+00, -2.4556e+00,  2.9673e-01, -4.2298e+00,  2.4787e+01,
        -1.1776e+01,  2.0292e+01, -9.4120e+00,  3.5687e+01,  6.3074e+01,
         6.3229e+01, -1.6595e+01,  8.8316e+00, -6.5371e+00, -2.1017e+01,
        -5.7385e+00,  4.7280e+00, -6.3658e+00,  1.3268e+01, -1.2851e+01,
        -6.1026e+01, -4.6877e+00, -2.0177e+01,  1.9680e+01,  2.1705e+01,
        -3.4645e+01, -1.5417e+01, -8.4611e+00,  5.8211e+00,  1.6696e+01,
         9.5188e+00,  2.5090e+01, -5.0460e-01,  1.4197e+01, -2.7520e+01,
        -2.2787e+01, -9.7312e+00,  2.2132e+01, -1.0685e+01,  2.0256e+01,
         4.5039e+00,  1.8620e+01, -1.5835e+01,  1.8767e+00, -4.1436e+01,
        -1.6944e-01, -7.8060e-01, -1.3627e+01, -8.1019e+00, -9.9509e+00,
        -6.1383e+00, -7.5134e-01], grad_fn=<SelectBackward0>)
```

## Task 04

**Description:**

Create a class `Head` to represent a single attention head.

Apply it on the output of `Task 3`. Print the shape of the result and the output for the first element in the first batch.

Set the seed of PyTorch to `123`.

**Acceptance criteria:**

1. A single self-attention head is implemented as shown in the Transformer paper and in `notes.md`.
2. The test case passes.
3. A seed is set for reproducibility.
4. An appropriate value is set for the parameter `head_size`.
5. All outputs to the console shown in the test case are present.

**Test case:**

```console
python task04.py
```

```console
k.shape=torch.Size([2, 4, 512])
q.shape=torch.Size([2, 4, 512])
v.shape=torch.Size([2, 4, 512])
attention_scores.shape=torch.Size([2, 4, 4])
Sum before softmax of first row of attention scores in the first batch: -40.34148406982422
Sum after softmax of first row of attention scores in the first batch: 1.0
Shape of output of head: torch.Size([2, 4, 512])
Output for the first element in the first batch: tensor([-20.2435,  10.7939,  -2.7727, -17.7557,   7.5533, -22.5912,  -1.7104,
        -18.4904,  14.8594,  -7.1789,   4.9455,  13.2950,  10.4287,  -2.9109,
          3.1468,  11.2634,  14.2564,   4.1825,  -8.2884, -23.3396,  -8.7246,
          5.5448,  -9.5699,  -4.7173, -10.1610,  -2.3163,  26.5402,  -9.8521,
         42.0578,   9.4863, -14.2835,   8.7397,  -0.6929, -17.7217,  26.3326,
         13.1659,   0.6596,  -9.7535,   7.5738,  -9.2377,  -8.9276,  -7.5475,
         36.8403,  -1.9660,  10.9039, -25.6739, -15.4309,  18.7954, -14.6519,
          0.1688,   4.7661,  13.2166,  -5.9902,  11.7940,  -0.2240,   4.5458,
         13.0475,  23.5205,   1.7142,  -4.4905, -12.6005,   6.7823,  29.9939,
        -23.1474,   9.8836,  -6.8831,  -9.0311,  10.3544, -10.6628,   9.2034,
         -8.3520,  -5.6248,  21.9185,  -6.7390,  -7.2990, -11.6634,  14.6032,
          7.9754,  -5.3964, -16.0555,   9.0009,  -1.6197,   3.3863,  -5.3541,
        -20.1798, -22.2645,  -2.3021,  22.4005, -15.2643,   0.5703, -17.2780,
         16.9925, -15.7048,   0.5085, -15.4980,   1.7507,  -3.2102,  25.6335,
          9.6527,   3.4301,  18.8066,   6.6937,   5.6441, -14.7036,  16.1985,
         -7.4656, -32.3314,  -5.9732, -12.3139, -21.6982, -23.7827,   8.0214,
         37.8511,  17.9366,  -0.7038,  10.7334, -11.5692,   8.0880,  -1.6167,
          5.5804,  17.6811,  -1.5055,  15.4537, -11.6441, -17.6108,  12.7979,
        -14.0387,  10.1177,  -8.3524,  15.6150, -11.2237,  -6.6714,   1.2903,
        -28.7336,  -6.4048, -14.0611, -29.9064,   2.3259,   6.1116, -20.0006,
          5.6489,  -2.7680,   6.4584,  27.6562, -30.0352,   1.4474,   7.8180,
          3.5112,   6.5977,  12.8861,  12.6220,  15.4899, -14.7072,   2.9907,
          5.5627,  12.8751,  13.6008,  16.2810,  -6.8804, -11.2391, -10.9414,
        -13.0222, -12.6125, -12.2434,   6.2276,  -0.5130, -14.2886, -11.1161,
          8.0873,   4.3194,   4.4996,  -6.0454,  -0.6726,  27.6270,  31.3520,
        -21.2198,  -8.9870,  -8.6558, -13.5370,  29.4735,   0.6124,   2.3546,
        -12.9986, -10.4097, -24.3950,   3.8516,  -5.5490,   9.5543, -16.2909,
         -5.0798,  -1.4455, -14.5005,   6.2972,   7.9109,  -5.3595,   4.0654,
        -15.2088,  -3.8260,  12.5033, -15.5969, -23.6385,   1.1099,  -0.2164,
         21.4404,  10.2535,  -2.2620,  -6.1266, -15.5201,  19.9272,  -1.9888,
         13.5510,  -3.4292,  16.8047,   2.5511, -28.6534,   5.7599,  12.5505,
          7.6446, -12.3488,   7.1180,  -0.3692,  -3.4601,  -8.8316, -10.0120,
         -4.1684,  19.9947,  -2.1897,  -2.2787, -15.1877,   3.6730,   1.2327,
          2.9802,  36.3362,  -3.2197,  12.5265,  19.5226, -30.0659,  -8.4560,
         -1.6607,  12.0148,  -4.6062,   8.0760,  -2.2086,  -5.5131,  11.2379,
        -16.6662,  25.6077,   7.7973,   4.8634,   1.8937, -11.3054,  17.3733,
         -4.2613,   3.2701,   0.4891,  25.6274, -27.6073,   1.7731,  -9.5969,
         -3.6507,  -3.6498,  -1.0619,  -8.6101,  10.4009, -15.8411, -16.1826,
        -12.4651,  -4.1681, -17.1173,   6.7129,  -5.5247,  11.6404,  -2.5402,
         -2.0534,  30.2107,  -9.9328, -11.6038,  -5.8831,  15.0899,   5.9029,
         -9.9199,   1.1925, -15.3272,   3.4237,  -4.2181,  -7.5480, -12.7899,
          2.2045,  12.8090,   1.2755, -18.1859,   2.6778,   2.5023,   6.9855,
        -12.5690, -14.6565,  -9.4139,  -8.0570,  -5.2475,  13.3696,  18.7882,
        -17.0290,  -8.7911, -19.3140, -17.2081, -19.4714,   3.4961,  -0.2654,
         19.7765,  19.7303, -31.6797,  10.5893,   5.0294,  -0.4432,  -8.4774,
          1.0435,  -5.0870, -21.7498,  -6.3714,  -1.8531, -10.2984, -14.8097,
        -36.6500,  11.9149,  16.5326, -18.0612,  -7.5028,   1.3655,   0.4682,
         13.7802, -25.1832, -27.4874,  -4.2078,  13.7758,  24.9664,  -0.6631,
         10.0050,  18.9367, -16.1941, -19.6875,  26.8835, -15.4949,  -2.7753,
         -2.6404,   9.6794, -14.3742,   1.1722,  -7.0388, -12.8949,  -7.2400,
         -8.8781, -14.7472,  23.0438, -21.1728,   1.6705,  -0.5758,  -7.4444,
        -20.3936,  12.7319, -14.0301, -17.3990,  22.8350,  -0.9383,  -6.9356,
        -13.3233,  -3.0503,  -0.4248, -11.6834,   1.1560,   2.4710, -16.8661,
          4.9018,   4.8929,  -8.0274,   9.0765,  -5.8257,  10.0613,  34.0701,
          4.3179, -14.8725,  35.3820,  -0.6258,  -4.2933,   6.5573,  -5.3838,
          1.9045,  16.9801,  -7.3662, -16.6491,   9.3844, -11.3313,  10.1898,
         15.2736,  17.2085, -20.6364, -25.4261,   0.8203,  16.0236,  -7.6905,
          3.8067, -11.8368,   8.5280,  -2.3472,   4.2038, -21.5176,   2.2633,
         14.4072, -14.3677,   0.2294,  -6.5777,  19.0644,   7.4114,  20.3860,
          6.3632,  28.5781,   7.4767,   5.9223,  -0.1267,  16.3867,   9.1548,
          0.4127,  16.6026, -22.6083,  -1.9268,   6.8651, -27.8572,  -0.6062,
        -15.0810,  -1.9614, -15.3538,  14.7630,   8.9751,  -5.1419,  19.2830,
          1.7885,   3.8604,  22.6474,  -5.5508,  -6.6358,   1.4042,   4.6098,
        -19.0044,  -7.4145,  -7.5559,  11.6565,   2.2391,  16.2645, -10.8802,
         11.4711,   1.0346,  -4.9290, -17.6626, -17.4796, -23.6363,   1.7568,
        -11.4486,  22.0778,  21.0798,  10.2329,  -7.3976,   8.2272,  -0.8877,
         13.2983, -11.7887, -22.8902,   4.2619,  10.4347,   4.5544,   2.8101,
         13.0159,   5.4364,  12.9673,  -9.8934,   4.5834,  -3.8188,  -1.1746,
         22.9956, -11.9203,   5.9656,  13.8014, -12.1082,  -8.8261,   6.8805,
         17.2075,  18.4439,  -4.5309,   7.2821, -11.7682,  16.7390,  -9.7074,
         16.4329, -27.0802,   2.7849,   4.1799, -10.2251, -15.6523,  -5.8546,
         10.8143, -10.9165,  -2.4753,   2.9221, -14.7643,  -1.4556, -12.8820,
         -5.8249,  18.0569, -22.4037,  -3.5013,  -0.8986,   3.1473, -11.3593,
         14.2268], grad_fn=<SelectBackward0>)
```

## Task 05

**Description:**

Extend the class to be able to apply masking on the padding tokens using a mask.

To make it easier to use and more reusable in general, move the logic for creating a mask given an input, into a function - `create_src_mask`.

Use the following input ($0$ means padding token):

```python
token_ids = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 9, 0, 0]])
```

Output:

- The shape of input before passing it to the function `create_src_mask`.
- The shape of the output of the function `create_src_mask`.
- The mask.
- The attention score before weighing them with the values.
- The shape of the output of the head.

Set the seed of PyTorch to `123`.

**Acceptance criteria:**

1. Masking is applied in the encoder.
2. The test case passes.
3. A seed is set for reproducibility.

**Test case:**

```console
python task05.py
```

```console
Input shape to "create_src_mask": torch.Size([3, 4])
Output shape of "create_src_mask": torch.Size([3, 4, 4])
Mask: tensor([[[1, 1, 1, 1],
         [1, 1, 1, 1],
         [1, 1, 1, 1],
         [1, 1, 1, 1]],

        [[1, 1, 1, 1],
         [1, 1, 1, 1],
         [1, 1, 1, 1],
         [1, 1, 1, 1]],

        [[1, 1, 0, 0],
         [1, 1, 0, 0],
         [1, 1, 0, 0],
         [1, 1, 0, 0]]])
Attention scores of the last batch: tensor([[0.8019, 0.1981, 0.0000, 0.0000],
        [0.8020, 0.1980, 0.0000, 0.0000],
        [0.4027, 0.5973, 0.0000, 0.0000],
        [0.4040, 0.5960, 0.0000, 0.0000]], grad_fn=<SelectBackward0>)
Shape of output of head: torch.Size([3, 4, 512])
```

## Task 06

**Description:**

Create a class `MultiHeadAttention` to represent multiple heads of self-attention running in parallel.

Use it to apply a `4`-headed self-attention using the input for `Task 5`. Print the shape of the result and the output for the first element in the first batch.

Output the shape of the output and the output in the first element in the first batch.

Set the seed of PyTorch to `123`.

**Acceptance criteria:**

1. Multi-head attention is implemented as shown in the Transformer paper and in `notes.md`.
2. The test case passes.
3. A seed is set for reproducibility.
4. An appropriate value is set for the parameter `head_size`.
5. All outputs to the console shown in the test case are present.

**Test case:**

```console
python task06.py
```

```console
Shape of output of multi-head attention: torch.Size([3, 4, 512])
Output for the first element in the first batch: tensor([ 5.1274e+00, -2.4552e+01, -1.1165e+01, -9.0528e+00,  1.9728e+01,
         1.5098e+01,  9.8812e-01, -2.0874e+00, -1.0894e+01,  1.2333e+01,
         3.1573e+00,  1.8208e+01,  1.1749e+01, -2.1726e+01,  1.2134e+01,
         1.1735e+01, -1.9230e+01,  7.2028e+00,  1.0925e+01, -2.2284e+00,
         3.0817e-01,  2.6503e+01,  3.2781e+00, -3.4599e+00, -6.0599e+00,
         1.1494e+01, -7.8940e+00, -2.3129e+01,  5.7649e+00, -1.9085e-01,
         3.5749e+00,  1.9287e+01, -6.3129e+00, -1.3850e+01,  2.3160e+01,
         1.9650e+01, -4.0899e-01, -1.6192e+01, -1.2805e+01,  2.5434e+01,
         1.8889e+01,  1.1747e+00, -1.2691e+01, -8.9250e+00,  1.0417e+01,
        -4.9367e+00, -1.2420e+01, -5.9968e+00, -1.4633e+01,  5.7509e-01,
         7.5981e+00, -1.3070e+01, -5.0652e+00, -2.1040e+00,  1.2381e+00,
         1.3731e+01, -6.4293e+00, -5.3785e+00,  9.0241e-01, -2.5017e+00,
        -4.8434e+00,  5.5195e+00, -2.8131e+01,  7.4639e+00, -1.4503e+01,
         1.2229e+01, -1.3450e+01,  1.4552e+01, -4.8573e+00,  1.8899e+01,
         1.0848e+01,  1.5537e+01,  3.2020e+01, -8.7013e-01, -3.2721e+01,
         1.2006e+01,  5.4876e+00,  5.8532e+00,  4.7876e+00,  2.1931e+01,
         1.0223e+01, -1.8331e+01,  3.1579e+01, -1.6827e+01,  1.5794e+01,
         1.2418e+01,  1.7882e+01,  1.9940e+01,  2.6559e+00, -2.9932e+01,
        -6.8548e+00, -7.9582e+00, -1.3746e+01,  1.0665e+01, -2.3241e+01,
         2.1477e+01, -3.6112e+00, -1.1921e+01,  1.7741e+01,  5.0333e+00,
        -4.0672e+01,  7.8918e+00, -1.6837e+01,  1.6227e+01, -1.4011e+01,
         2.2261e+01, -9.7862e+00,  8.6968e+00, -1.3707e+01,  1.4296e+01,
        -8.0751e-01,  2.4447e+00,  2.8520e+01, -2.5341e+00,  4.6394e+00,
         3.5927e-01, -1.4490e+00,  7.8483e+00,  5.4541e+00,  2.0212e+00,
        -4.9461e+00, -4.8245e+00, -9.5891e+00,  2.6200e+01, -1.3629e+00,
         1.2910e+01, -8.0822e+00, -5.5648e+00, -1.3601e+01, -7.0584e+00,
         5.5552e+00, -1.1293e+01,  2.2337e+01, -2.6379e+01,  3.0036e+01,
         2.3577e+01, -1.4009e+01, -3.2387e+01,  1.2340e+01, -5.8406e+00,
         4.1393e-01,  3.0988e+00, -9.5549e+00,  1.6655e+01,  1.2502e+01,
         1.4727e+01, -2.0126e+01, -1.6879e+01, -1.9078e+00, -7.1052e+00,
        -1.2457e+01, -3.5361e+00, -1.9893e+01,  4.1171e+00, -1.4683e+01,
        -4.0161e+00, -2.1461e+01, -1.1417e+01, -9.2205e+00,  2.8661e+01,
         1.2772e+01, -6.2559e+00, -2.5410e+01,  9.0711e+00,  1.0330e+01,
         1.7396e-01,  1.0188e+01, -1.1262e+01,  7.0083e+00, -1.7305e+00,
        -7.5324e-02, -4.2354e+00, -4.8020e+00,  1.5467e+01,  4.4684e+00,
         4.1569e+00, -6.1173e+00, -2.1929e+01,  5.2595e+00,  1.6829e+00,
         7.4608e+00, -6.5008e+00,  1.0277e+01,  5.7303e+00,  5.7284e+00,
        -2.8905e+01, -1.2254e+01,  1.9015e+01, -8.4026e+00,  6.2800e+00,
        -6.7130e+00,  9.0365e-01,  2.0465e+01,  3.0713e+00,  1.7261e+00,
         1.0535e+01, -9.9298e+00, -4.2618e-01, -1.4789e+01,  1.0491e+01,
        -8.9820e+00, -1.4931e+01, -1.4325e+01,  1.8281e+01,  3.0416e+00,
         6.3856e+00, -1.7153e+01, -5.9974e+00,  1.8453e+01, -2.9488e+01,
         7.3822e+00, -2.4116e+01,  3.9963e+00, -1.3667e+01, -5.6582e+00,
         1.1491e+01,  6.2156e+00, -9.7432e+00,  8.8405e+00, -1.0301e+01,
        -1.4526e+00,  5.9509e+00, -6.8676e+00, -9.5444e-02, -2.5350e+00,
        -6.6628e+00, -5.5506e+00,  4.5340e-01,  3.1648e+00, -2.8103e-01,
        -1.0955e+01,  8.6958e+00, -2.2992e+01,  1.4166e+01, -3.0997e+01,
         7.6119e+00,  2.0289e+01,  1.1783e+01, -5.8904e+00, -1.1280e+01,
        -1.9097e+01,  1.1729e-02,  4.6318e+00, -1.6106e+01, -2.0194e+01,
         1.2085e+00, -8.0206e-01, -5.0432e+00,  4.7226e+00, -5.6650e+00,
        -1.4367e+01, -9.9197e+00,  2.1637e+01,  3.5722e+00,  1.0056e+01,
         9.0207e+00,  2.8865e+01,  1.4005e+00, -1.1905e+01,  1.8539e+01,
         4.1114e+00,  5.1377e+00, -1.4914e+01, -1.4538e+01,  5.8343e+00,
        -2.1044e+01,  1.0045e+00, -1.0632e+00, -6.5213e+00,  1.9641e+01,
         7.1017e+00, -5.9845e+00,  9.5474e+00, -5.8778e+00,  1.7064e+01,
        -4.3395e+00, -1.0663e+01,  1.8352e+01,  3.7588e+00,  1.2076e+01,
         1.1574e+01, -1.3928e+01,  1.7668e+01, -1.1384e+01, -2.8128e+01,
         1.0824e+01, -6.7729e+00, -4.1132e+00, -1.2068e+01, -1.0577e+01,
        -1.3391e+01,  8.2701e+00, -2.0266e+01,  2.2702e+01, -1.8989e+01,
         9.5782e+00, -8.0613e+00, -1.0166e+01,  6.3266e+00, -7.5459e+00,
         2.3265e+00,  3.8272e+00,  1.0583e+01, -7.7807e+00,  1.2448e+01,
         5.2744e+00, -2.3307e+01, -5.5345e+00, -2.6091e+00, -2.5185e+01,
         4.9444e+00, -9.9008e+00,  6.7048e+00, -5.0361e+00, -1.1549e+01,
         1.0680e+01,  8.4199e+00, -8.1910e+00,  2.3627e+01, -2.7745e+01,
         8.9184e+00,  8.5735e+00,  8.4529e+00, -6.1102e+00,  1.6481e+01,
        -5.2806e+00, -4.1853e+00, -1.6939e+01, -3.7569e+00, -7.2510e+00,
        -4.0591e+00,  5.6349e+00, -1.4366e+01,  8.6807e+00, -5.7888e+00,
         8.9493e+00,  2.7582e+00,  7.9382e+00, -1.1230e+01, -4.8325e+00,
         4.7019e+00, -5.8449e+00,  8.1299e+00,  1.0974e+01, -8.0549e+00,
        -8.3348e+00, -3.0047e+01,  6.8776e+00, -5.9328e+00, -3.0627e+00,
        -7.2611e+00,  1.9637e+00,  5.5048e-01, -3.1850e+00, -1.1087e+01,
        -5.9797e+00,  1.7857e+01, -1.2893e+01,  4.4301e+00, -1.3251e+01,
         2.2433e+01,  4.5263e+00, -8.2927e+00,  2.2718e+01, -1.3875e+01,
        -3.6178e+00, -3.2804e+01,  2.7868e+00, -3.8776e+00,  1.6546e+00,
        -2.0732e+01, -1.3436e+01,  1.1717e+00, -7.8959e+00, -8.3858e-01,
         3.7964e+01, -9.3265e+00,  5.3412e+00,  6.2530e+00,  7.4899e+00,
        -9.6649e+00, -1.2074e+01,  2.0112e+01, -1.0861e+01, -1.0179e+01,
        -6.8485e+00, -7.7082e+00, -8.7215e+00,  5.7715e+00, -1.1899e+01,
        -9.9434e+00,  1.1872e+00,  2.2071e+00,  1.3740e+01,  2.7311e+01,
         1.2195e+01, -1.1846e+01, -3.9628e+00, -3.0562e+01,  5.1104e+00,
        -2.4293e+00, -5.4454e+00, -3.7311e-01,  3.3930e+00, -5.7269e+00,
        -6.3582e-01,  2.2390e+01,  1.4731e+01,  3.1227e+00, -7.5189e+00,
        -9.3276e+00,  1.4429e+01,  1.3773e+01, -1.6188e+01, -5.9436e+00,
        -1.3657e+01,  9.9133e+00, -2.2036e+01,  2.8109e+01,  7.8619e+00,
         2.7136e+00, -4.0628e-01,  9.5261e-01, -1.1301e+01,  1.1382e+01,
        -2.9866e+00, -1.0106e+01,  1.2962e+01, -4.7618e+00, -2.4971e+01,
        -8.3472e+00, -2.0739e+01,  8.2772e+00, -1.8230e+01,  3.8192e+00,
         5.2862e+00,  1.7687e+01, -5.4530e+00,  1.3427e+01,  1.9574e+01,
        -9.7095e+00,  4.6136e+00, -4.2116e+00,  3.3271e+01, -1.7807e+01,
         8.4097e+00, -9.4743e+00, -5.4267e-01,  1.3532e+01, -3.2612e+00,
        -1.7294e+01,  1.5396e+00,  8.7776e+00, -1.1213e+01,  7.1361e+00,
        -2.7113e+01, -1.0063e+01,  8.9850e+00, -1.4485e+00, -5.9191e+00,
        -2.2283e+01, -1.2024e+00,  1.1852e+00, -1.4735e+01,  2.3954e+00,
        -2.7452e+01,  3.0301e+00,  1.1509e+01,  3.7082e+00, -1.2860e+01,
        -1.4407e+01,  4.5387e+00, -9.9563e+00,  9.2004e+00,  1.1300e+01,
         9.8215e+00,  6.3145e+00,  7.0312e+00, -2.2220e+01, -5.2029e+00,
         9.3300e+00, -2.1162e+00, -3.1629e-01,  7.2086e+00, -8.0674e-01,
        -7.5604e+00, -5.1831e+00,  1.5473e+00,  1.2279e+01, -1.1163e+00,
         5.1627e-01,  3.5982e+00, -1.5775e+00, -1.0011e+01,  2.1098e+01,
        -8.0512e+00, -5.1514e+00, -1.7293e+01,  4.8256e+00,  2.4143e+01,
         1.6669e+01, -1.7175e+01, -1.0311e+01,  6.6831e+00, -7.7997e+00,
        -2.6526e+00, -2.2875e+01,  1.0899e+01, -8.7882e+00,  1.0031e+01,
        -4.3700e+00,  5.1113e+00], grad_fn=<SelectBackward0>)
```

## Task 07

**Description:**

Define a class `FeedForwardSubLayer` that represents the feed-forward part of the Transformer architecture.

Apply it on the output of `Task 6`, using a hidden dimension of `2048`.

Output the shape of the input to the layer and the shape of the output of the layer.

Set the seed of PyTorch to `123`.

**Acceptance criteria:**

1. The layer is implemented as shown in the Transformer paper and in `notes.md`.
2. The test case passes.
3. A seed is set for reproducibility.
4. All outputs to the console shown in the test case are present.

**Test case:**

```console
python task07.py
```

```console
Input shape: torch.Size([3, 4, 512])
Output shape: torch.Size([3, 4, 512])
```

## Task 08

**Description:**

Define a class `EncoderLayer` that represents a single encoder layer. Use a dropout rate of $0.1$.

Output:

- the shape of the input to the layer;
- the shape of the output of the layer;
- the first element in the first batch.

Set the seed of PyTorch to `123`.

**Acceptance criteria:**

1. The layer is implemented as shown in the Transformer paper and in `notes.md`.
2. The test case passes.
3. A seed is set for reproducibility.
4. All outputs to the console shown in the test case are present.

**Test case:**

```console
python task08.py
```

```console
Input shape: torch.Size([3, 4, 512])
Output shape: torch.Size([3, 4, 512])
First element in first batch: tensor([-0.9940, -0.3030,  0.7459,  0.6215, -0.0216,  1.9300, -0.2925, -0.2691,
         0.0651, -0.8870,  1.0694,  1.5211,  0.9777, -1.6285,  0.3791,  0.5482,
         3.0068,  0.0283, -1.4528, -0.6155,  2.5228,  0.1918, -0.6945,  0.2167,
         0.7959, -0.8715,  1.2930, -1.5992, -0.7582, -1.8074, -0.8931,  0.1026,
         2.2220, -0.1003,  1.0310, -0.0992,  0.8127,  0.2467, -0.3914, -1.3614,
        -0.9781, -0.1746, -0.0266, -0.3403, -1.5589, -0.7880, -1.2344, -0.5174,
         0.2044,  0.6177, -0.3700,  0.1045, -1.1105, -0.5557,  1.8714, -2.5442,
        -0.6593, -1.4106, -0.4674,  0.2244, -0.3992, -0.3007,  1.2508, -2.0312,
         0.7243,  0.1848,  1.1484,  0.2393, -0.3060, -0.8223, -1.1239,  0.5097,
         0.9110, -0.2078, -0.1577, -1.4244,  0.0196,  0.2178, -0.1294,  0.0673,
         0.0960,  0.5214, -1.2008, -0.8306,  1.6590,  1.5952, -0.0544,  0.4586,
         0.6495,  0.5684, -0.9729, -0.7648,  0.1655,  0.2620, -1.7280, -3.6275,
        -1.1973,  0.3559,  0.3898, -1.3126,  0.2157,  1.9887, -0.1331,  1.0185,
         1.1921, -0.2461, -1.0244, -0.8167,  0.0479,  1.0503,  0.5428, -0.2740,
        -1.3470,  0.9129,  0.7237,  0.4232, -2.2008,  0.0098,  0.1365, -1.1871,
         0.4793,  2.2644, -0.5217, -0.0160,  1.8462, -1.0517,  0.6630,  0.5780,
        -0.6761,  0.5808, -0.1635, -1.1684,  0.4639, -1.1781, -1.0807, -0.3689,
         1.8254,  0.1226,  0.0217,  0.7952,  1.5975,  0.1500,  0.1987, -0.2359,
        -2.0521, -1.3356,  0.3488, -0.1574, -0.5276,  0.8045, -0.0357, -0.2870,
        -0.7617,  1.3033, -0.5838,  0.9789,  0.0727, -0.6899,  1.3235,  1.0922,
         1.9382, -0.2264, -0.6810,  1.2968,  0.0396, -0.5168,  0.0284, -1.1682,
        -0.2363,  1.6365, -0.3606,  0.1706,  1.0379,  3.4377, -0.7388,  1.0716,
         0.0237, -1.4495, -1.5550,  0.6208, -0.4525,  0.5667, -0.4362, -0.4025,
        -0.5571, -0.0288,  0.0446, -0.6734, -0.3371, -0.0807, -0.5698,  0.7746,
        -0.0353, -1.6822, -1.3285,  0.2151, -0.5349, -1.5767, -0.9389, -1.6921,
         0.1361, -0.6451, -0.8494, -1.3843, -0.4259,  1.0563, -0.6326, -0.5368,
         0.5221,  0.5385,  1.3102, -1.5757,  0.6940,  0.1976, -0.4549, -0.0143,
         1.4081,  1.2563, -1.4295, -1.3033,  0.0607, -0.2385, -0.4154,  0.8645,
        -0.7668,  0.3990,  0.7104, -0.7484,  1.2502,  0.3172, -0.0192,  0.2275,
        -0.0373, -0.0952,  1.0934,  0.6826,  0.9860, -0.3523,  0.2676,  0.4060,
        -0.0594,  0.6296,  0.0356,  0.5078, -0.6172,  1.2904,  0.5482, -1.1128,
         0.5334, -1.6233, -0.1695,  0.2290,  0.6538,  0.5233,  0.0317,  0.7319,
         0.7272,  1.7837,  0.2092,  0.8976, -1.3334,  0.5697, -0.1560,  0.8335,
        -1.3748, -1.1677, -0.0330, -0.7507,  0.9534, -0.8712,  0.0050, -0.2419,
         2.0710,  0.0828,  0.6706,  0.2876, -0.6671,  1.0587, -0.1609,  0.7162,
        -0.8039,  0.1678, -0.0531, -0.8922,  0.1116, -1.0247, -1.3272, -0.4587,
         1.3751,  0.9432,  1.0055, -0.0530,  1.2172,  0.1235,  1.4708, -0.7083,
        -1.5863,  0.0991, -0.5688,  1.1338,  0.2254, -1.4365, -0.3805,  1.1566,
        -1.8070, -2.2024,  1.3392,  1.2440, -1.0263, -0.2749, -0.4019,  1.3270,
         0.7326,  1.4291,  0.4021,  0.6581,  0.6891, -1.9741, -0.9016, -0.1237,
        -1.8855,  0.3350,  0.8774,  0.7527,  0.1795,  0.2974,  0.3199,  0.3595,
         1.2744, -0.0452,  0.3475, -1.5947,  1.1458,  1.1859,  0.2793,  0.5551,
         0.3269,  0.0882,  1.2359, -0.1293,  0.4972,  0.1649,  0.6559,  0.0053,
        -0.3687, -1.5920, -1.3973, -0.6020, -0.9440, -2.4444,  0.4755, -2.0602,
         0.7888, -0.9859, -0.5418,  1.7146,  0.2890,  0.5259,  0.8998,  1.0082,
         1.8567, -0.5247, -0.6385,  0.9704, -0.1318,  1.0953,  0.0702,  1.1239,
        -0.8923, -1.1543,  0.7120,  0.5068, -0.2686,  0.4972,  1.9753, -1.6130,
         0.8186, -1.5799, -0.4877, -1.3800, -0.0912, -0.6208, -1.5642, -0.3876,
        -0.0912,  1.0143, -2.0085, -0.4623, -1.5589,  0.1064, -0.9404,  2.7710,
        -0.4050,  0.3627,  0.6843, -0.0187,  0.9256, -0.9280, -0.9922,  0.2373,
        -0.6128,  0.1437, -0.3719,  0.9981,  1.2374,  0.3773,  1.2193, -0.4416,
        -1.3442,  0.2526,  0.4519,  0.8053, -0.3190,  2.1343, -1.9279,  0.1814,
        -2.0913, -0.4188,  1.1697,  1.1014,  0.0821,  0.1213, -0.5094, -0.5395,
         0.2660,  0.1974, -1.4684,  0.2251, -0.5083, -1.1558,  0.5086,  0.2819,
        -0.9330,  0.9519, -0.6322,  1.9317, -1.1373,  0.5150,  1.4105, -0.0072,
         0.0813,  1.2665,  0.0426, -0.8079, -0.7778, -0.6055, -0.6950, -0.5040,
        -0.3431, -0.5918, -1.2838,  0.9083, -0.8897,  0.3447, -1.6261, -0.4538,
         2.8394, -0.2003, -0.6431,  2.6221, -0.3423, -0.1759,  0.2798, -1.0303,
         1.4588, -0.1879,  0.6464, -0.6837,  1.5681,  2.7159,  2.7102, -0.5897,
         0.2782, -0.0947, -0.9534, -0.0991, -0.0232, -0.2599,  1.0153, -0.8324,
        -2.3750, -0.9453, -0.9407,  0.7688,  0.6591, -1.9056, -0.8911,  0.3993,
        -0.1179,  1.2082,  0.2277,  2.2883,  0.8943,  0.3513, -0.8287, -0.7997,
        -0.7698,  1.0275,  0.0662,  0.5523,  0.1668,  0.6770,  0.0126,  0.2791,
        -1.0113,  0.6499,  0.3278, -0.8562, -0.5363, -0.2686, -0.6781,  0.3365],
       grad_fn=<SelectBackward0>)
```

## Task 09

**Description:**

Define a class `TransformerEncoder` that represents an encoder block of several encoder layers.

Set the:

- vocabulary size to $10,000$;
- embedding dimension to $512$;
- number of encoder layers to $4$;
- number of attention heads to $4$;
- hidden dimension of the feed-forward layer to $2,048$;
- dropout rate to $0.1$;
- maximum sequence length to $4$.

Output:

- the shape of the input to the block;
- the shape of the output of the block;
- the first element in the first batch.

Set the seed of PyTorch to `123`.

**Acceptance criteria:**

1. The block is implemented as shown in the Transformer paper and in `notes.md`.
2. The test case passes.
3. A seed is set for reproducibility.
4. All outputs to the console shown in the test case are present.

**Test case:**

```console
python task09.py
```

```console
Input shape: torch.Size([3, 4])
Output shape: torch.Size([3, 4, 512])
First element in first batch: tensor([-7.1292e-01,  4.0623e-01,  7.0920e-01,  1.2279e+00, -2.1799e-01,
         9.7099e-01, -8.9486e-01, -7.3388e-01, -5.8739e-02, -1.9794e-01,
         1.2154e+00,  1.4147e+00,  3.8944e-01, -5.7622e-01, -8.5842e-01,
         4.9006e-01,  2.4274e+00, -6.7096e-02, -6.2474e-01, -1.1601e+00,
         1.3080e+00,  1.2091e+00, -1.8019e+00,  2.2611e-01,  3.0079e-02,
         9.0149e-02,  1.9694e+00, -1.2590e+00, -4.1046e-01, -8.0502e-01,
         2.7043e-01,  1.0946e-01,  1.4137e+00, -1.6867e-01,  1.1856e+00,
        -4.1478e-01, -2.8247e-01,  5.5379e-01, -2.4656e-01, -6.4297e-01,
        -3.1750e-01,  1.7582e-02,  2.2143e-01, -3.3194e-01, -5.1126e-01,
        -7.4294e-02, -1.1330e+00, -2.8265e-01, -1.5662e-01,  4.6227e-01,
         4.3164e-01, -4.5248e-01, -4.0267e-01, -1.4857e+00,  1.3377e+00,
        -1.4499e+00, -1.5711e-01,  1.3311e-01,  4.5196e-01, -6.7302e-01,
        -7.1703e-01,  1.7525e-01,  1.3483e+00, -2.4953e+00,  2.3362e-01,
         2.7920e-02,  2.5124e+00, -3.6624e-01, -7.4505e-01, -1.9555e+00,
         3.0694e-01,  1.6556e-02,  9.9082e-01,  5.9849e-01, -4.1049e-01,
        -1.2910e+00, -4.9062e-01, -2.0487e-01,  3.6599e-02, -7.3434e-01,
        -5.1739e-01,  1.2192e+00, -6.4112e-01, -5.4612e-01,  1.6230e+00,
         1.7876e+00, -1.8915e-02, -6.5649e-01,  1.6688e+00,  9.0012e-01,
        -3.5481e-01, -1.2286e-01,  3.8647e-02,  2.9498e-01, -3.4892e-01,
        -3.4852e+00, -6.2901e-01, -2.8427e-01,  6.9698e-01, -2.0581e+00,
         2.5278e-02,  1.7977e+00, -1.0670e-01,  8.7403e-01,  1.8602e+00,
         3.1943e-01, -1.1956e+00, -6.2474e-01,  3.3221e-01,  7.9340e-01,
         8.8386e-02, -3.3841e-01, -1.2718e+00,  8.1780e-01,  2.0527e-01,
         4.9171e-02, -4.7608e-01,  8.3098e-01,  1.4397e-01, -1.7051e+00,
        -1.1197e+00,  2.0745e+00, -4.8633e-01,  5.2632e-01,  2.2752e+00,
        -2.0254e-01, -2.0520e-01, -1.3838e-01, -7.1920e-01,  1.9549e-01,
         6.4332e-01, -1.6568e+00,  5.4210e-01,  2.2656e-01, -3.9303e-01,
        -5.5743e-01,  2.1532e+00, -8.1704e-01, -2.5042e-01,  2.1467e-01,
         8.6086e-01,  1.7322e+00, -9.2121e-01, -4.3217e-01, -1.8468e+00,
         2.4783e-01,  1.6423e+00, -9.1484e-01, -5.1619e-01,  3.3700e-01,
         8.2182e-01, -1.2523e+00, -1.2108e+00,  8.7516e-01, -2.7485e-01,
         8.5546e-01,  1.8981e-01,  1.6688e-01,  1.3299e+00,  1.6547e+00,
         1.8616e+00,  4.0620e-01, -1.3541e+00,  1.5507e+00,  3.2446e-01,
        -8.3584e-02,  7.3058e-01,  3.6744e-01, -5.7638e-01,  2.1123e+00,
         6.8725e-01,  1.0808e+00,  6.1258e-02,  3.1966e+00, -1.3580e+00,
         2.1923e+00,  4.6834e-01, -1.5773e+00, -1.6964e+00,  4.8090e-01,
        -3.4092e-01,  8.4031e-01,  5.9607e-01, -4.0506e-01, -7.1411e-01,
        -6.9539e-01,  2.0087e-01, -1.4465e+00,  7.5792e-01,  2.3831e-01,
        -8.1299e-01, -1.2938e-03,  1.8869e-01, -1.1532e+00, -2.0056e+00,
         2.1176e+00, -1.2280e+00, -1.5907e+00, -6.4196e-01, -6.9448e-01,
        -2.9575e-01, -1.0386e+00,  4.9073e-03, -1.1157e+00,  4.8630e-01,
         1.7219e-01,  2.9771e-01, -2.3523e-01, -1.6548e-01,  8.3278e-01,
         1.7160e+00, -4.4234e-01,  1.1837e+00,  8.4955e-01, -3.4278e-01,
         1.2127e+00,  1.2000e+00,  1.9542e+00, -1.2415e+00, -1.5852e+00,
        -9.4422e-02, -5.8460e-01, -1.2104e+00,  9.0516e-01, -6.1351e-01,
        -8.5425e-01,  1.2626e+00, -5.2970e-01, -8.3956e-02, -3.1996e-01,
        -4.0329e-01, -5.3673e-01,  1.4553e-01, -3.6168e-01,  1.4996e+00,
         3.2407e-02,  2.0211e+00,  2.7293e-01,  6.7501e-02,  6.8491e-01,
        -4.5335e-01, -3.8680e-01, -1.6279e+00,  6.2152e-01,  1.1154e-02,
         1.7429e+00,  1.5973e+00, -6.1819e-01,  1.5422e+00, -1.4534e+00,
         5.5809e-01, -1.3025e+00,  5.7939e-01,  6.0120e-01, -7.0344e-02,
        -1.2096e+00, -1.3110e+00,  1.0908e+00,  2.8052e-01,  3.5830e-01,
        -1.0710e+00,  1.3799e+00, -3.2434e-01,  6.4106e-01, -6.7627e-01,
        -7.9449e-01,  6.1740e-01, -4.7211e-02,  1.6410e+00, -6.4678e-01,
         9.2873e-02, -8.7043e-01,  1.9160e-01, -9.1442e-01,  1.2798e+00,
         1.8097e+00, -3.3206e-01,  1.3277e-01, -8.7154e-01,  3.0223e-01,
        -9.4329e-01,  9.4283e-01, -5.5224e-01, -1.2398e+00,  2.9902e-01,
         6.3104e-01, -1.0849e+00, -2.8226e-01,  9.0855e-01,  2.8042e-01,
         1.7219e+00, -1.4005e-01,  1.3688e+00,  4.3527e-01,  3.7300e-01,
        -1.1172e-01, -2.0524e+00,  2.4791e-01, -2.7572e-01,  5.2766e-01,
        -8.7029e-01, -8.2698e-01, -4.6987e-01,  8.4950e-01, -1.8197e+00,
        -5.2348e-01,  7.8664e-01,  1.7192e+00, -1.4436e-01,  6.7440e-01,
        -5.9001e-01, -7.5389e-01, -2.4768e-01,  3.2564e-01,  7.6366e-01,
        -2.3004e-01,  9.1482e-01, -1.2631e+00, -7.8022e-01,  2.2514e-01,
        -1.2233e+00, -9.1565e-01,  1.4602e+00,  2.0282e+00, -7.0617e-01,
         1.4409e+00,  2.1793e-01, -7.4794e-01,  7.8085e-01,  6.7361e-01,
         3.5158e-02, -2.3807e+00,  1.8937e+00,  5.9446e-01,  3.6305e-01,
         3.5840e-01, -3.2738e-01,  2.8109e-02,  5.5981e-01, -3.9707e-01,
        -1.1700e+00, -9.1174e-01, -3.0066e-01, -1.0168e+00,  2.7029e-01,
         5.7621e-01, -6.8572e-01, -2.0554e-01, -8.2576e-01, -1.2013e+00,
         7.4009e-01,  4.6090e-01,  5.9238e-01, -1.9069e+00, -2.2343e-01,
         1.3804e+00,  5.2075e-01,  7.7152e-02,  9.7770e-01,  9.8082e-01,
         6.0204e-01, -2.8157e-01, -3.6079e-01,  1.3010e+00, -2.7750e-02,
        -4.9888e-01,  5.1653e-01, -4.1726e-01, -7.0270e-02, -1.2754e+00,
        -3.6682e-01,  3.0707e-01, -4.1967e-01,  1.7960e-01,  1.2127e+00,
        -1.0399e+00,  3.5434e-01, -4.7219e-01, -6.8690e-01, -9.4861e-01,
        -1.0925e+00,  2.5452e-01, -1.7092e+00, -3.0708e-01, -1.6151e-01,
         1.2331e+00, -1.4928e+00,  2.8991e-01, -2.2987e+00,  3.2454e-02,
        -7.3931e-01,  1.6139e+00, -5.9382e-01, -1.1189e-01,  6.8807e-01,
        -8.7604e-01, -1.6749e-01, -1.7604e-01, -9.8591e-01, -3.9311e-01,
        -2.0796e-01, -3.8446e-01,  8.4839e-01, -2.0222e-01,  2.1887e+00,
         5.5351e-01,  8.9738e-01, -8.2058e-01, -8.4852e-01,  1.2753e+00,
         3.2685e-01, -3.0022e-01, -1.3970e+00,  2.4268e+00, -1.6083e+00,
        -5.2791e-01, -1.1884e+00, -4.0339e-02,  1.3671e+00,  1.8110e-01,
        -9.6101e-01, -7.0348e-01, -8.9781e-01, -2.2274e-01,  6.4049e-01,
         9.1530e-01, -2.4417e+00,  2.9651e-01, -1.5898e+00, -3.8119e-01,
         7.6213e-01,  1.1125e-01, -7.9459e-01,  3.4567e-01, -1.0774e+00,
         1.1172e+00, -1.8749e+00,  1.5150e-01,  2.0856e+00, -3.6765e-01,
        -8.9675e-01,  4.5816e-01, -2.7820e-01, -1.5873e+00, -8.2133e-01,
         1.1646e-01, -3.9421e-01, -2.1171e+00, -8.3495e-01, -1.2498e+00,
        -1.4884e+00, -5.2538e-01, -1.1370e+00, -1.1526e-01, -7.6359e-02,
         1.0731e-01,  2.5054e+00, -1.0526e+00, -4.2157e-01,  1.1835e+00,
         1.5997e-01,  7.6518e-01,  5.0501e-01, -1.4068e+00,  7.3355e-01,
        -2.8244e-01,  1.0187e+00, -1.3370e+00,  1.4290e+00,  3.2137e+00,
         1.3847e+00, -4.1642e-01,  1.8032e+00,  1.1532e+00, -2.8024e-01,
        -1.0859e+00, -5.6481e-01,  1.7558e-01,  1.1417e+00, -5.3369e-01,
        -2.6758e+00,  1.3257e-01, -8.2462e-01,  3.4735e-01,  4.0220e-01,
        -9.9976e-01, -1.4759e+00,  7.7658e-01,  1.2507e+00,  2.6428e-01,
         9.1648e-01,  8.2408e-01,  1.2975e+00, -3.6198e-01, -5.2922e-01,
        -4.2220e-01, -1.3084e+00,  6.1628e-01,  4.6833e-01,  4.1100e-01,
         6.6516e-01, -6.7400e-01, -6.4866e-01,  2.3650e-01, -1.5032e+00,
         5.6458e-01,  4.8632e-01, -1.4359e+00,  1.8841e-01,  9.7781e-01,
         3.8443e-02,  6.1675e-01], grad_fn=<SelectBackward0>)
```

## Task 10

**Description:**

Define a class `ClassifierHead` that represents a the classification head of an encoder-only Transformer, applying [nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#logsoftmax) before returning.

Set the:

- vocabulary size to $10,000$;
- embedding dimension to $512$;
- number of encoder layers to $6$;
- number of attention heads to $8$;
- hidden dimension of the feed-forward layer to $2,048$;
- dropout rate to $0.1$;
- maximum sequence length to $4$;
- number of possible output classes to $3$.

Output:

- the outputs for the batches;
- the shape of the output;
- the shape of the output of the classification head.

Set the seed of PyTorch to `123`.

**Acceptance criteria:**

1. The classification head is implemented as discussed in `notes.md`.
2. The test case passes.
3. A seed is set for reproducibility.
4. All outputs to the console shown in the test case are present.

**Test case:**

```console
python task10.py
```

```console
Classification outputs for a batch of 3 sequences:
tensor([[[-1.0047, -0.9712, -1.3657],
         [-1.1736, -1.1551, -0.9789],
         [-1.7757, -0.8411, -0.9178],
         [-0.8309, -1.0251, -1.5818]],

        [[-1.1511, -0.7982, -1.4543],
         [-1.4439, -0.4831, -1.9165],
         [-0.6508, -0.9922, -2.2292],
         [-0.9610, -1.0601, -1.3054]],

        [[-1.9286, -1.1532, -0.6180],
         [-1.6908, -1.3641, -0.5798],
         [-1.3095, -0.9528, -1.0660],
         [-1.2530, -1.3412, -0.7923]]], grad_fn=<LogSoftmaxBackward0>)
Encoder output shape: torch.Size([3, 4, 512])
Classification head output shape: torch.Size([3, 4, 3])
```

## Task 11

**Description:**

Implement a decoder-only Transformer, including all needed layers. Apply it on the following input:

```python
input_ids = [[6044, 8239, 4933, 3760, 8963, 8379, 5427, 8503, 3497, 5683],
        [4101, 6866, 2756, 1399, 5878,  376,   56, 9868, 8794, 6033]]
```

Set the:

- vocabulary size to $10,000$;
- embedding dimension to $512$;
- number of decoder layers to $6$;
- number of attention heads to $8$;
- hidden dimension of the feed-forward layer to $2,048$;
- dropout rate to $0.1$;
- maximum sequence length to $20$.

Output:

- the result;
- the shape of the result;
- the most likely next token for each of the batches.

Set the seed of PyTorch to `123`.

**Acceptance criteria:**

1. A decoder-only transformer is implemented as discussed in `notes.md`.
2. The test case passes.
3. A seed is set for reproducibility.
4. All outputs to the console shown in the test case are present.

**Test case:**

```console
python task11.py
```

```console
tensor([[[-10.0155,  -8.8104,  -9.5242,  ...,  -9.9904,  -8.6582,  -8.8317],
         [ -9.5170,  -8.3810,  -9.2627,  ..., -10.1876,  -8.8069,  -8.9152],
         [ -9.4973,  -9.2950,  -9.1033,  ...,  -9.8614,  -8.1184,  -8.9226],
         ...,
         [ -9.8524,  -8.4217,  -9.2957,  ...,  -9.8019,  -8.9438,  -8.9274],
         [ -9.9180,  -8.0020,  -9.1149,  ..., -10.6107,  -8.6750,  -8.5358],
         [ -8.9729,  -8.5063,  -9.4987,  ..., -10.4141,  -8.7239,  -8.3831]],

        [[ -9.3646,  -8.1904,  -9.2207,  ...,  -9.6919,  -8.7994,  -9.8138],
         [ -9.5010,  -9.0448,  -9.4254,  ...,  -9.2910,  -9.1079,  -9.4723],
         [-10.2538,  -8.7304,  -8.2604,  ...,  -9.7464,  -8.9539,  -9.7006],
         ...,
         [ -8.9150,  -8.7760,  -9.3612,  ...,  -8.6978,  -9.6172,  -9.6029],
         [ -9.5015,  -8.9602,  -9.0253,  ...,  -8.9481,  -9.4054,  -9.8821],
         [ -8.8316,  -9.1429,  -9.2725,  ...,  -9.2585,  -9.9521, -10.2231]]],
       grad_fn=<LogSoftmaxBackward0>)
torch.Size([2, 10, 10000])
Most likely next token batch 1: 6318
Most likely next token batch 2: 2334
```

## Task 12

**Description:**

Implement a full encoder-decoder Transformer, including modifications to all needed layers. Apply it on the following input:

```python
input_ids = [[6044, 8239, 4933, 3760, 8963, 8379, 5427, 8503, 3497, 5683],
        [4101, 6866, 2756, 1399, 5878,  376,   56, 9868, 8794, 6033]]
```

Set the:

- vocabulary size to $10,000$;
- embedding dimension to $512$;
- number of decoder layers to $6$;
- number of attention heads to $8$;
- hidden dimension of the feed-forward layer to $2,048$;
- dropout rate to $0.1$;
- maximum sequence length to $20$.

Output:

- the result;
- the shape of the result.

Set the seed of PyTorch to `123`.

**Acceptance criteria:**

1. An encoder-decoder transformer is implemented as discussed in `notes.md` and as shown in the Transformer paper.
2. The test case passes.
3. A seed is set for reproducibility.
4. All outputs to the console shown in the test case are present.

**Test case:**

```console
python task12.py
```

```console
tensor([[[-10.4375,  -8.4803,  -9.0915,  ...,  -9.1372,  -9.8855,  -9.9463],
         [-10.2101,  -8.5006,  -9.3736,  ...,  -8.7776,  -9.5935,  -9.3735],
         [-10.2009,  -8.1255,  -9.0285,  ...,  -9.5153, -10.1802,  -9.4276],
         ...,
         [ -9.7870,  -8.4316,  -8.8871,  ...,  -8.7105, -10.0035,  -9.4517],
         [-10.4181,  -7.9421,  -9.2179,  ...,  -9.2300,  -9.7104,  -9.4585],
         [-10.1037,  -8.2540,  -9.1262,  ...,  -9.4837, -10.0786,  -9.0111]],

        [[ -8.8077,  -8.9159,  -9.7612,  ...,  -9.3616,  -8.7374, -10.8247],
         [ -9.2600,  -8.6116,  -9.4530,  ...,  -9.4817,  -8.7172, -10.3816],
         [ -9.1755,  -8.2524,  -9.1585,  ...,  -9.6463,  -9.0681, -10.3511],
         ...,
         [ -9.1694,  -8.8855, -10.1033,  ...,  -9.3384,  -8.6760, -10.8035],
         [ -9.3009,  -8.4309,  -9.8151,  ...,  -9.4837,  -9.1687, -10.3537],
         [ -9.0157,  -8.2889,  -9.9568,  ...,  -9.5164,  -9.0823, -10.4598]]],
       grad_fn=<LogSoftmaxBackward0>)
torch.Size([2, 10, 10000])
```
